/****************************************************************************
**  SCALASCA    http://www.scalasca.org/                                   **
*****************************************************************************
**  Copyright (c) 1998-2016                                                **
**  Forschungszentrum Juelich GmbH, Juelich Supercomputing Centre          **
**                                                                         **
**  Copyright (c) 2009-2013                                                **
**  German Research School for Simulation Sciences GmbH,                   **
**  Laboratory for Parallel Programming                                    **
**                                                                         **
**  This software may be modified and distributed under the terms of       **
**  a BSD-style license.  See the COPYING file in the package base         **
**  directory for details.                                                 **
****************************************************************************/


/**
@page start Getting started

This chapter provides an introduction to the use of the Scalasca Trace
Tools on the basis of the analysis of an example application. The most
prominent features are addressed, and at times a reference to later chapters
with more in-depth information on the corresponding topic is given.

Use of the Scalasca Trace Tools involves three phases: instrumentation of
the target application, execution measurement collection and analysis, and
examination of the analysis report. For instrumentation and measurement, the
Scalasca Trace Tools 2.x release series leverages the Score-P
infrastructure, while the Cube graphical user interface is used for analysis
report examination. Scalasca complements the functionality provided by Cube
and Score-P with scalable automatic trace-analysis components, as well as
convenience commands for controlling execution measurement collection and
analysis, and analysis report postprocessing.

Most of Scalasca's functionality can be accessed through the `scalasca`
command, which provides action options that in turn invoke the corresponding
underlying commands `scorep`, `scan` and `square`. These actions are:

 1. `scalasca -instrument`

    (or short `skin`) familiar to users of the Scalasca 1.x series is
    <b>deprecated</b> and only provided for backward compatibility. It tries
    to map the command-line options of the Scalasca 1.x instrumenter onto
    corresponding options of Score-P's instrumenter command `scorep` -- as
    far as this is possible. However, to take full advantage of the improved
    functionality provided by Score-P, <b>users are strongly encouraged to
    use the `scorep` instrumenter command directly</b>. Please refer to the
    Score-P User Manual @cite scorep_manual for details. To assist in
    transitioning existing measurement configurations to Score-P, the
    Scalasca instrumentation wrapper prints the converted command that is
    actually executed to standard output.

 2. `scalasca -analyze`

    (or short `scan`) is used to control the Score-P measurement environment
    during the execution of the target application (supporting both runtime
    summarization and/or event trace collection, optionally including
    hardware-counter information), and to automatically initiate Scalasca's
    trace analysis after measurement completion if tracing was requested.

 3. `scalasca -examine`

    (or short `square`) is used to postprocess the analysis report generated
    by a Score-P profiling measurement and/or Scalasca's automatic
    post-mortem trace analysis, and to start the analysis report examination
    browser Cube.

To get a brief usage summary, call the `scalasca` command without arguments,
or use `scalasca --quickref` to open the Scalasca Quick Reference (with a
suitable PDF viewer).

@note
    Under the hood, the Scalasca convenience commands leverage a number of
    other commands provided by Score-P and Cube. Therefore, it is generally
    advisable to include the executable directories of appropriate
    installations of all three components in the shell search path (`PATH`).

The following three sections provide a quick overview of each of these
actions and how to use them during the corresponding step of the performance
analysis, before a tutorial-style full workflow example is presented in
Section @ref start_workflow.

@seclist
    @secitem{start_instrumentation}
    @secitem{start_measurement}
    @secitem{start_examination}
    @secitem{start_workflow}
@endseclist

@navigation_prev{intro}
*/


//---------------------------------------------------------------------------

/**
@page start_instrumentation Instrumentation

To generate measurements which can be used as input for the Scalasca Trace
Tools, user application programs first need to be instrumented. That is,
special measurement calls have to be inserted into the program code which are
then executed at specific important points (events) during the application
run. Unlike previous versions of Scalasca which used a custom measurement
system, this task is now accomplished by the community instrumentation and
measurement infrastructure Score-P.

As already mentioned in the previous section, use of the `scalasca
-instrument` and `skin` commands is discouraged, and therefore not discussed
in detail. Instead, all the necessary instrumentation of user routines,
OpenMP constructs and MPI functions should be handled by the Score-P
instrumenter, which is accessed through the `scorep` command. Therefore, the
compile and link commands to build the application that is to be analyzed
should be prefixed with `scorep` (e.g., in a Makefile).

For example, to instrument the MPI application executable `myapp` generated
from the two Fortran source files `foo.f90` and `bar.f90`, the following
compile and link commands

@verbatim
  % mpif90 -c foo.f90
  % mpif90 -c bar.f90
  % mpif90 -o myapp foo.o bar.o
@endverbatim

have to be replaced by corresponding commands using the Score-P instrumenter:

@verbatim
  % scorep  mpif90 -c foo.f90
  % scorep  mpif90 -c bar.f90
  % scorep  mpif90 -o myapp foo.o bar.o
@endverbatim

This will automatically instrument every routine entry and exit seen by the
compiler, intercept MPI function calls to gather message-passing information,
and link the necessary Score-P measurement libraries to the application
executable.

@attention
    <b>The `scorep` instrumenter must be used with the link command</b> to
    ensure that all required Score-P measurement libraries are linked with
    the executable. However, not all object files need to be instrumented,
    thereby avoiding measurements and data collection for routines and OpenMP
    constructs defined in those files. <b>Instrumenting files defining OpenMP
    parallel regions is essential</b>, as Score-P has to track the
    creation of new threads.

Although generally most convenient, automatic compiler-based function
instrumentation as used by default may result in too many and/or too
disruptive measurements, which can be addressed with selective
instrumentation and measurement filtering. While the most basic steps will be
briefly covered in Section @ref start_workflow_filtering, please refer to the
Score-P manual @cite scorep_manual for details on the available
instrumentation and filtering options.

@navigation_next{start_measurement}
*/

//---------------------------------------------------------------------------

/**
@page start_measurement Runtime measurement collection & analysis

While applications instrumented by Score-P can be executed directly with a
measurement configuration defined via environment variables, the `scalasca
-analyze` (or short `scan`) convenience command provided by Scalasca can be
used to control certain aspects of the Score-P measurement environment during
the execution of the target application. To produce a performance measurement
using an instrumented executable, the target application execution command is
prefixed with the `scalasca -analyze` (or short `scan`) command:

@verbatim
  % scalasca -analyze [options] \
             [<launch_cmd> [<launch_flags>]] <target> [target args]
@endverbatim

For pure MPI or hybrid MPI+OpenMP applications, @a launch_cmd is typically
the MPI execution command such as `mpirun` or `mpiexec`, with @a launch_flags
being the corresponding command-line arguments as used for uninstrumented
runs, e.g., to specify the number of compute nodes or MPI ranks. For non-MPI
(i.e., serial and pure OpenMP) applications, the launch command and
associated flags can usually be omitted.

In case of the example MPI application executable `myapp` introduced in the
previous section, a measurement command starting the application with four
MPI ranks could therefore be:

@verbatim
  % scalasca -analyze  mpiexec -n 4 ./myapp
@endverbatim

@attention
    A unique directory is used for each measurement experiment, which must
    not already exist when measurement starts: otherwise measurement is
    aborted immediately.

A default name for the experiment directory is composed of the prefix
&quot;`scorep_`&quot;, the target application executable name, the run
configuration (e.g., number of MPI ranks and `OMP_NUM_THREADS`), and a few
other parameters of the measurement configuration. For example, a measurement
of the `myapp` application as outlined above will produce a measurement
experiment directory named &quot;`scorep_myapp_4_sum`&quot;. Alternatively,
the experiment directory name can also be explicitly specified with the `-e
<experiment_name>` option of `scalasca -analyze` or via the environment
variable `SCOREP_EXPERIMENT_DIRECTORY`.

@note
    A number of settings regarding the measurement configuration can be
    specified in different ways. Command-line options provided to `scalasca
    -analyze` always have highest precedence, followed by Score-P environment
    variables, and finally automatically determined defaults.

When measurement has completed, the measurement experiment directory contains
various log files and one or more analysis reports. By default, runtime
summarization is used to provide a summary report of the number of visits and
time spent on each callpath by each process/thread, as well as hardware
counter metrics (if configured). For MPI or hybrid MPI+OpenMP applications,
MPI message statistics are also included.

Event trace data can also be collected as part of a measurement. This
measurement mode can be enabled by passing the `-t` option to the `scalasca
-analyze` command (or alternatively by setting the environment variable
`SCOREP_ENABLE_TRACING` to 1).

@note
    Enabling event trace collection does not automatically turn off
    summarization mode (i.e., both a summary profile and event traces are
    collected). It has to be explicitly disabled when this behavior is
    undesired.

When collecting a trace measurement, experiment trace analysis is
automatically initiated after measurement is complete to quantify wait states
that cannot be determined with runtime summarization. In addition to
examining the trace-analysis report, the generated event traces can also be
visualized with a third-party graphical trace browser such as Vampir
@cite knuepfer_ea:2008.

@warning
    <b>Traces can easily become extremely large and unwieldy</b>, and
    uncoordinated intermediate trace buffer flushes may result in cascades of
    distortion, which renders such traces to be of little value. <b>It is
    therefore extremely important to set up an adequate measurement
    configuration <i>before</i> initiating trace collection and analysis!</b>
    Please see Section @ref start_workflow_filtering as well as the Score-P
    User Manual @cite scorep_manual for more details on how to set up a
    filtering file and adjust Score-P's internal memory management.

@navigation_both{start_instrumentation,start_examination}
*/


//---------------------------------------------------------------------------

/**
@page start_examination Analysis report examination

The results of the runtime summarization and/or the automatic trace analysis
are stored in one or more reports (i.e., CUBE4 files) in the measurement
experiment directory. These reports can be postprocessed and examined using
the `scalasca -examine` (or short `square`) command, providing an experiment
directory name as argument:

@verbatim
  % scalasca -examine [options] <experiment_name>
@endverbatim

Postprocessing is done the first time an experiment is examined, before
launching the Cube analysis report browser. If the `scalasca -examine`
command is provided with an already processed experiment directory, or with a
CUBE4 file specified as argument, the viewer is launched immediately.

Instead of interactively examining the measurement analysis results, a
textual score report can also be obtained using the `-s` option without
launching the viewer:

@verbatim
  % scalasca -examine -s <experiment_name>
@endverbatim

This score report comes from the `scorep-score` utility and provides a
breakdown of the different types of regions included in the measurement and
their estimated associated trace buffer capacity requirements, aggregate
trace size and largest process trace buffer size (`max_buf`), which can be used to
set up a filtering file and to determine an appropriate `SCOREP_TOTAL_MEMORY`
setting for a subsequent trace measurement. See Section
@ref start_workflow_filtering for more details.

The Cube viewer can also be directly used on an experiment archive -- opening
a dialog window to choose one of the contained CUBE4 files -- or an
individual CUBE4 file as shown below:

@verbatim
  % cube <experiment_name>
  % cube <file>.cubex
@endverbatim

However, keep in mind that no postprocessing is performed in this case, so
that only a subset of Scalasca's analyses and metrics may be shown.

@navigation_both{start_measurement,start_workflow}
*/


//---------------------------------------------------------------------------

/**
@page start_workflow A full workflow example

While the previous sections introduced the general usage workflow of Scalasca
based on an abstract example, we will now guide through an example analysis
of a moderately complex benchmark code using MPI: BT from the NAS Parallel
Benchmarks (NPB-MPI 3.3) @cite npb_website. The BT benchmark implements a
simulated CFD application using a block-tridiagonal solver for a synthetic
system of nonlinear partial differential equations and consists of about 20
Fortran 77 source code files. Although BT does not exhibit significant
performance bottlenecks -- after all, it is a highly optimized benchmark --
it serves as a good example to demonstrate the overall workflow, including
typical configuration steps and how to avoid common pitfalls.

The example measurements (available for download on the Scalasca
documentation web page @cite scalasca_webdocs) were carried out using
Scalasca in combination with Score-P @scorep_version and Cube @cube_version
on the JUROPA cluster at J&uuml;lich Supercomputing Centre. JUROPA's compute
nodes are equipped with two Intel Xeon X5570 (Nehalem-EP) quad-core CPUs
running at 2.93 GHz, and connected via a QDR Infiniband fat-tree network. The
code was compiled using Intel compilers and linked against ParTec ParaStation
MPI (which is based on MPICH2). The example commands shown below -- which are
assumed to be available in `PATH`, e.g., after loading site-specific
environment modules -- should therefore be representative for using Scalasca
in a typical HPC cluster environment.

@note
    Remember that the Scalasca commands use other commands provided by
    Score-P and Cube. It is assumed that the executable directories of
    appropriate installations of all three components are available in the
    shell search path.

@seclist
    @secitem{start_workflow_reference}
    @secitem{start_workflow_instrument}
    @secitem{start_workflow_initial_summary}
    @secitem{start_workflow_filtering}
    @secitem{start_workflow_summary}
    @secitem{start_workflow_trace}
@endseclist
@navigation_prev{start_examination}
*/


//---------------------------------------------------------------------------

/**
@page start_workflow_reference Preparing a reference execution

As a first step of every performance analysis, a reference execution using an
uninstrumented executable should be performed. On the one hand, this step
verifies that the code executes cleanly and produces correct results, and on
the other hand later allows to assess the overhead introduced by
instrumentation and measurement. At this stage an appropriate test
configuration should be chosen, such that it is both repeatable and long
enough to be representative. (Note that excessively long execution durations
can make measurement analysis inconvenient or even prohibitive, and therefore
should be avoided.)

After unpacking the NPB-MPI source archive, the build system has to be
adjusted to the respective environment. For the NAS benchmarks, this is
accomplished by a Makefile snippet defining a number of variables used by a
generic Makefile. This snippet is called `make.def` and has to reside in the
`config/` subdirectory, which already contains a template file that can be
copied and adjusted appropriately. In particular, the MPI Fortran compiler
wrapper and flags need to be specified, for example:

@verbatim
  MPIF77     = mpif77
  FFLAGS     = -O2
  FLINKFLAGS = -O2
@endverbatim

Note that the MPI C compiler wrapper and flags are not used for building BT,
but may also be set accordingly to experiment with other NPB benchmarks.

Next, the benchmark can be built from the top-level directory by running
`make`, specifying the number of MPI ranks to use (for BT, this is required
to be a square number) as well as the problem size on the command line:

@verbinclude start_make.log

Valid problem classes (of increasing size) are W, S, A, B, C, D and E, and
can be used to adjust the benchmark runtime to the execution environment. For
example, class W or S is appropriate for execution on a single-core laptop
with 4 MPI ranks, while the other problem sizes are more suitable for
&quot;real&quot; configurations.

The resulting executable encodes the benchmark configuration in its name and
is placed into the `bin/` subdirectory. For the example `make` command above,
it is named `bt.D.64`. This binary can now be executed, either via submitting
an appropriate batch job (which is beyond the scope of this user guide) or
directly in an interactive session.

@verbinclude start_reference.log

Note that this application verified its successful calculation and reported
the associated wall-clock execution time for the core computation.

@navigation_next{start_workflow_instrument}
*/

//---------------------------------------------------------------------------

/**
@page start_workflow_instrument Instrumenting the application code

Now that the reference execution was successful, it is time to prepare an
instrumented executable using Score-P to perform an initial measurement. By
default, Score-P leverages the compiler to automatically instrument every
function entry and exit. This is usually the best first approach, when you
don't have detailed knowledge about the application and need to identify the
hotspots in your code. For BT, using Score-P for instrumentation is simply
accomplished by prefixing the compile and link commands specified in the
`config/make.def` Makefile snippet by the Score-P instrumenter command
`scorep`:

@verbatim
  MPIF77     = scorep mpif77
@endverbatim

Note that the linker specification variable `FLINK` in `config/make.def`
defaults to the value of `MPIF77`, i.e., no further modifications are
necessary in this case.

Recompilation of the BT source code in the top-level directory now creates an
instrumented executable, overwriting the uninstrumented binary (for archiving
purposes, one may consider renaming it before recompiling):

@verbinclude start_instrument.log

@navigation_both{start_workflow_reference,start_workflow_initial_summary}
*/


//---------------------------------------------------------------------------

/**
@page start_workflow_initial_summary Initial summary measurement

The instrumented executable prepared in the previous step can now be executed
under the control of the `scalasca -analyze` (or short `scan`) convenience
command to perform an initial summary measurement:

@verbinclude start_initial.log

As can be seen, the measurement run successfully produced an experiment
directory `scorep_bt_64_sum` containing
 - the runtime summary result file `profile.cubex`,
 - a copy of the measurement configuration in `scorep.cfg`, and
 - the measurement log file `scorep.log`.

However, application execution took about twice as long as the reference run
(940.81 vs. 462.95 seconds). That is, instrumentation and associated measurements
introduced a non-negligible amount of overhead. While it is possible to
interactively examine the generated summary result file using the Cube report
browser, this should only be done with great caution since the substantial
overhead negatively impacts the accuracy of the measurement.

@navigation_both{start_workflow_instrument,start_workflow_filtering}
*/


//---------------------------------------------------------------------------

/**
@page start_workflow_filtering Optimizing the measurement configuration

To avoid drawing wrong conclusions based on skewed performance data due to
excessive measurement overhead, it is often necessary to optimize the
measurement configuration before conducting additional experiments. This can
be achieved in various ways, e.g., using runtime filtering, selective
recording, or manual instrumentation controlling measurement. Please refer to
the Score-P Manual @cite scorep_manual for details on the available options.
However, in many cases it is already sufficient to filter a small number of
frequently executed but otherwise unimportant user functions to reduce the
measurement overhead to an acceptable level. The selection of those routines
has to be done with care, though, as it affects the granularity of the
measurement and too aggressive filtering might &quot;blur&quot; the location
of important hotspots.

To help identifying candidate functions for runtime filtering, the initial
summary report can be scored using the `-s` option of the `scalasca -examine`
command:

@verbinclude start_score.log

As can be seen from the top of the score output, the estimated size for an
event trace measurement without filtering applied is apprximately 3.7 TiB,
with the process-local maximum across all ranks being roughly 62 GB (~58
GiB). Considering the 24 GiB of main memory available on the JUROPA compute
nodes and the 8 MPI ranks per node, a tracing experiment with this
configuration is clearly prohibitive if disruptive intermediate trace buffer
flushes are to be avoided.

The next section of the score output provides a table which shows how the
trace memory requirements of a single process (column `max_buf`) as well as
the overall number of visits and CPU allocation time are distributed among
certain function groups. Currently, the following groups are distinguished:

 - <b>`MPI`</b>:
   MPI API functions.

 - <b>`OMP`</b>:
   OpenMP constructs and API functions.

 - <b>`COM`</b>:
   User functions/regions that appear on a call path to an OpenMP construct,
   or an OpenMP or MPI API function. Useful to provide the context of
   MPI/OpenMP usage.

 - <b>`USR`</b>:
   User functions/regions that do not appear on a call path to an OpenMP
   construct, or an OpenMP or MPI API function.

The detailed breakdown by region below the summary provides a classification
according to these function groups (column `type`) for each region found in
the summary report. Investigation of this part of the score report reveals
that most of the trace data would be generated by about 50 billion calls to
each of the three routines `matvec_sub`, `matmul_sub` and `binvcrhs`, which
are classified as `USR`. And although the percentage of time spent in these
routines at first glance suggest that they are important, the average time
per visit is below 250 nanoseconds (column `time/visit`). That is, the
relative measurement overhead for these functions is substantial, and thus a
significant amount of the reported time is very likely spent in the Score-P
measurement system rather than in the application itself. Therefore, these
routines constitute good candidates for being filtered (like they are good
candidates for being inlined by the compiler). Additionally selecting the
`exact_solution` routine, which generates 447 MB of event data on a single
rank with very little runtime impact, a reasonable Score-P filtering file
would therefore look like this:

@verbinclude npb-bt.filt

Please refer to the Score-P User Manual @cite scorep_manual for a detailed
description of the filter file format, how to filter based on file names,
define (and combine) blacklists and whitelists, and how to use wildcards for
convenience.

The effectiveness of this filter can be examined by scoring the initial
summary report again, this time specifying the filter file using the `-f`
option of the `scalasca -examine` command. This way a filter file can be
incrementally developed, avoiding the need to conduct many measurements to
step-by-step investigate the effect of filtering individual functions.

@verbinclude start_score_filt.log

Below the (original) function group summary, the score report now also
includes a second summary with the filter applied. Here, an additional group
`FLT` is added, which subsumes all filtered regions. Moreover, the column
`flt` indicates whether a region/function group is filtered
(&quot;`+`&quot;), not filtered (&quot;`-`&quot;), or possibly partially filtered
(&quot;`*`&quot;, only used for function groups).

As expected, the estimate for the aggregate event trace size drops down to
3.3 GiB, and the process-local maximum across all ranks is reduced to 53 MiB.
Since the Score-P measurement system also creates a number of internal data
structures (e.g., to track MPI requests and communicators), the suggested
setting for the `SCOREP_TOTAL_MEMORY` environment variable to adjust the
maximum amount of memory used by the Score-P memory management is 55 MiB when
tracing is configured (see Section @ref start_workflow_trace).

@navigation_both{start_workflow_initial_summary,start_workflow_summary}
*/


//---------------------------------------------------------------------------

/**
@page start_workflow_summary Summary measurement & examination

The filtering file prepared in Section @ref start_workflow_filtering can now
be applied to produce a new summary measurement, ideally with reduced
measurement overhead to improve accuracy. This can be accomplished by
providing the filter file name to `scalasca -analyze` via the `-f` option.

@attention
    Before re-analyzing the application, the unfiltered summary experiment
    should be renamed (or removed), since `scalasca -analyze` will not
    overwrite the existing experiment directory and abort immediately.

@verbinclude start_summary.log

Notice that applying the runtime filtering reduced the measurement overhead
significantly, down to now only 3% (477.66 seconds vs. 462.95 seconds for the
reference run). This new measurement with the optimized configuration should
therefore accurately represent the real runtime behavior of the BT
application, and can now be postprocessed and interactively explored using
the Cube result browser. These two steps can be conveniently initiated using
the `scalasca -examine` command:

@verbinclude start_summary_exam.log

Examination of the summary result (see Figure @ref SummaryExperiment for a
screenshot and Section @ref start_workflow_cube for a brief summary of how to
use the Cube browser) shows that 97% of the overall CPU allocation time is
spent executing computational user functions, while 2.6% of the time is spent
in MPI point-to-point communication functions and the remainder scattered
across other activities. The point-to-point time is almost entirely spent in
`MPI_Wait` calls inside the three solver functions `x_solve`, `y_solve` and
`z_solve`, as well as an `MPI_Waitall` in the boundary exchange routine
`copy_faces`. Execution time is also mostly spent in the solver routines and
the boundary exchange, however, inside the different `solve_cell`,
`backsubstitute` and `compute_rhs` functions. While the aggregated time spent
in the computational routines seems to be relatively balanced across the
different MPI ranks (determined using the box plot view in the right pane),
there is quite some variation for the `MPI_Wait` / `MPI_Waitall` calls.

@figure{start_summary.png,SummaryExperiment,Screenshot of a summary experiment result in the Cube report browser.,width=\textwidth}


@section start_workflow_cube Using the Cube browser

The following paragraphs provide a very brief introduction to the usage of
the Cube analysis report browser. To make effective use of the GUI, however,
please also consult the Cube User Guide @cite cube_manual.

Cube is a generic user interface for presenting and browsing performance and
debugging information from parallel applications. The underlying data model
is independent from particular performance properties to be displayed. The
Cube main window (see Figure @ref SummaryExperiment) consists of three panels
containing tree displays or alternate graphical views of analysis reports.
The left panel shows performance properties of the execution, such as time or
the number of visits. The middle pane shows the call tree or a flat profile
of the application. The right pane either shows the system hierarchy
consisting of, e.g., machines, compute nodes, processes, and threads, a
topological view of the application's processes and threads (if available),
or a box plot view showing the statistical distribution of values across the
system. All tree nodes are labeled with a metric value and a color-coded box
which can help in identifying hotspots. The metric value color is determined
from the proportion of the total (root) value or some other specified
reference value, using the color scale at the bottom of the window.

A click on a performance property or a call path selects the corresponding
node. This has the effect that the metric value held by this node (such as
execution time) will be further broken down into its constituents in the
panels right of the selected node. For example, after selecting a performance
property, the middle panel shows its distribution across the call tree. After
selecting a call path (i.e., a node in the call tree), the system tree shows
the distribution of the performance property in that call path across the
system locations. A click on the icon to the left of a node in each tree expands or
collapses that node. By expanding or collapsing nodes in each of the three
trees, the analysis results can be viewed on different levels of granularity
(inclusive vs. exclusive values).

All tree displays support a context menu, which is accessible using the right
mouse button and provides further options. For example, to obtain the exact
definition of a performance property, select "Online Description" in the
context menu associated with each performance property. A brief description
can also be obtained from the menu option "Info".

@navigation_both{start_workflow_filtering,start_workflow_trace}
*/


//---------------------------------------------------------------------------

/**
@page start_workflow_trace Trace collection and analysis

While summary profiles only provide process- or thread-local data aggregated
over time, event traces contain detailed time-stamped event data which also
allows to reconstruct the dynamic behavior of an application. This enables
tools such as the Scalasca trace analyzer to provide even more insights into
the performance behavior of an application, for example, whether the time
spent in MPI communication is real message processing time or incurs
significant wait states (i.e., intervals where a process sits idle without
doing useful work waiting for data from other processes to arrive).

Trace collection and subsequent automatic analysis by the Scalasca trace
analyzer can be enabled using the `-t` option of `scalasca -analyze`. Since
this option enables trace collection <i>in addition</i> to collecting a
summary measurement, it is often used in conjunction with the `-q` option
which turns off measurement entirely. (Note that the order in which these two
options are specified matters.)

@attention
    <b>Do not forget to specify an appropriate measurement configuration</b>
    (i.e., a filtering file and `SCOREP_TOTAL_MEMORY` setting)! Otherwise,
    you may easily fill up your disks and suffer from uncoordinated
    intermediate trace buffer flushes, which typically render such traces to
    be of little (or no) value!

For our example measurement, scoring of the initial summary report in Section
@ref start_workflow_filtering with the filter applied estimated a total
memory requirement of 55 MiB per process (which could be verified by
re-scoring the filtered summary measurement). As this exceeds the default
`SCOREP_TOTAL_MEMORY` setting of 16 MiB, use of the prepared filtering file
alone is not yet sufficient to avoid intermediate trace buffer flushes. In
addition, the `SCOREP_TOTAL_MEMORY` setting has to be adjusted accordingly
before starting the trace collection and analysis. (Alternatively, the
filtering file could be extended to also exclude the `binvrhs` routine from
measurement.) Note that renaming or removing the summary experiment directory
is not necessary, as trace experiments are created with suffix
&quot;`trace`&quot;.

@verbinclude start_trace.log

After successful trace collection and analysis, the generated experiment
directory `scorep_bt_64_trace` contains the measurement configuration
`scorep.cfg`, the measurement log file `scorep.log` and a copy of the
filtering file `scorep.filt`. In addition, an OTF2 trace archive is produced,
consisting of the anchor file `traces.otf2`, the global definitions file
`traces.def` and the per-process data in the `traces/` directory. Finally,
the experiment also includes the trace analysis reports `scout.cubex` and
`trace.stat`, as well as a log file storing the output of the trace analyzer
(`scout.log`).

The Scalasca trace analyzer also warned about a number of point-to-point
clock condition violations it detected. A clock condition violation is a
violation of the logical event order that can occur when the local clocks of
the individual compute nodes are insufficiently synchronized. For example, a
receive operation may appear to have finished before the corresponding send
operation started -- something that is obviously not possible. The Scalasca
trace analyzer includes a correction algorithm @cite becker_ea:2009 that can
be applied in such cases to restore the logical event order, while trying to
preserve the length of intervals between local events in the vicinity of the
violation.

To enable this correction algorithm, the `--time-correct` command-line option
has to be passed to the Scalasca trace analyzer. However, since the analyzer
is implicitly started through the `scalasca -analyze` command, this option
has to be set using the `SCAN_ANALYZE_OPTS` environment variable, which holds
command-line options that `scalasca -analyze` should forward to the trace
analyzer.  An existing trace measurement can be re-analyzed using the `-a`
option of the `scalasca -analyze` command, avoiding the need to collect a new
experiment:

@verbinclude start_trace_correct.log

@note
    The additional time required to execute the timestamp correction
    algorithm is typically small compared to the trace data I/O time and
    waiting times in the batch queue for starting a second analysis job. On
    platforms where clock condition violations are likely to occur (i.e.,
    clusters), it is therefore often convenient to enable the timestamp
    correction algorithm by default.

Similar to the summary report, the trace analysis report can finally be
postprocessed and interactively explored using the Cube report browser, again
using the `scalasca -examine` convenience command:

@verbinclude start_trace_exam.log

The report generated by the Scalasca trace analyzer is again a profile in
CUBE4 format, however, enriched with additional performance properties.
Examination shows that roughly half of the time spent in MPI point-to-point
communication is waiting time, equally split into <i>Late Sender</i> and
<i>Late Receiver</i> wait states (see Figure @ref TraceAnalysis). While the
execution time in the `solve_cell` routines looked relatively balanced in the
summary profile, examination of the <i>Critical path imbalance</i> metric
shows that these routines in fact exhibit a small amount of imbalance, which
is likely to cause the wait states at the next synchronization point. This
can be verified using the <i>Late Sender delay costs</i> metric, which
confirms that the `solve_cells` as well as the `y_backsubstitute` and
`z_backsubstitute` routines are responsible for almost all of the <i>Late
Sender</i> wait states. Likewise, the <i>Late Receiver delay costs</i> metric
shows that the majority of the <i>Late Receiver</i> wait states are caused by
the `solve_cells` routines as well as the `MPI_Wait` calls in the solver
routines, where the latter indicates a communication imbalance or an
inefficient communication pattern.

@figure{start_trace.png,TraceAnalysis,Screenshot of a trace analysis result in the Cube report browser.,width=\textwidth}

@navigation_prev{start_workflow_summary}
*/
